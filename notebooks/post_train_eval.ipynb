{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helpers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import lightning as L\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from helpers.dataset import get_dataloaders\n",
    "from helpers.diffusion import get_diffusion\n",
    "from helpers.model import WeightDiffusionTransformer\n",
    "from helpers.pl_module import WeightDenoiser\n",
    "from helpers.texture_encoding import GramEncoder, CLIP, VisionTransformer\n",
    "from helpers.texture_loss import TextureLoss\n",
    "\n",
    "from helpers.generator import Generator as nca_weight_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ckpt(model_id) -> str:\n",
    "    artifact = wandb.Api().artifact(f'ludekcizinsky/hypernca/{model_id}')\n",
    "    artifact.download()\n",
    "\n",
    "def get_pl_module(model_id, artifact_dir, username='cizinsky', model_type='newest'):\n",
    "\n",
    "    # download ckpt from wandb based on model_id\n",
    "    download_ckpt(model_id)\n",
    "\n",
    "    # Get first path to checkpoint\n",
    "    model_dir = f'{artifact_dir}/{model_id}'\n",
    "    path_to_ckpt = os.path.join(model_dir, \"model.ckpt\")\n",
    "\n",
    "    # Load config\n",
    "    ckpt = torch.load(path_to_ckpt, map_location='cpu', weights_only=False)\n",
    "    cfg = ckpt['hyper_parameters']\n",
    "    if model_type == 'baseline':\n",
    "        default_cfg = OmegaConf.load('../configs/train.yaml')\n",
    "        cfg = OmegaConf.merge(default_cfg, cfg)\n",
    "\n",
    "    # Adjust the config\n",
    "    OmegaConf.set_struct(cfg, False)\n",
    "    cfg.data.nca_weights_path = f'/scratch/izar/{username}/hypernca/pretrained_nca/Flickr+DTD_NCA'\n",
    "    cfg.model.type = model_type\n",
    "    if model_type == 'baseline':\n",
    "        cfg.model.use_cross_attention = False\n",
    "\n",
    "    # Load all the other components\n",
    "    _, val_dataloader, normaliser = get_dataloaders(cfg)\n",
    "\n",
    "    diffusion = get_diffusion(cfg)\n",
    "    model = WeightDiffusionTransformer(cfg)\n",
    "\n",
    "    if \"Gram\" in cfg.texture_encoder._target_:\n",
    "        encoder = GramEncoder(hidden_size=cfg.texture_encoder.hidden_size, normalize=cfg.texture_encoder.normalize)\n",
    "    elif \"CLIP\" in cfg.texture_encoder._target_:\n",
    "        encoder = CLIP()\n",
    "    elif \"VisionTransformer\" in cfg.texture_encoder._target_:\n",
    "        encoder = VisionTransformer(\n",
    "            pretrained=cfg.texture_encoder.pretrained,\n",
    "            trainable=cfg.texture_encoder.trainable,\n",
    "            num_hidden_layers=cfg.texture_encoder.num_hidden_layers,\n",
    "            patch_size=cfg.texture_encoder.patch_size,\n",
    "            hidden_dim=cfg.texture_encoder.hidden_dim,\n",
    "            num_layers=cfg.texture_encoder.num_layers,\n",
    "            num_heads=cfg.texture_encoder.num_heads,\n",
    "            mlp_dim=cfg.texture_encoder.mlp_dim,\n",
    "            image_size=cfg.texture_encoder.image_size,\n",
    "        )\n",
    "\n",
    "    pl_module = WeightDenoiser(cfg=cfg, model=model, diffusion=diffusion, normaliser=normaliser,encoder=encoder).to('cuda')\n",
    "    pl_module.load_state_dict(ckpt['state_dict'], strict=True)\n",
    "\n",
    "    return pl_module, encoder, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post training testing of the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_id = 'model-sdnv2k1z:v0'\n",
    "compare_id = 'model-4kkhlc27:v3'\n",
    "artifact_dir = '/home/cizinsky/x-to-nif/notebooks/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-sdnv2k1z:v0, 391.15MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.0\n",
      "/home/cizinsky/venvs/hypernca/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "baseline, base_encoder, _ = get_pl_module(baseline_id, artifact_dir, model_type='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-4kkhlc27:v3, 910.55MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 151.28M\n"
     ]
    }
   ],
   "source": [
    "comparison_model, comp_encoder, val_dataloader = get_pl_module(compare_id, artifact_dir, model_type='newest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cizinsky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cizinsky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "weights2image_gen = nca_weight_generator()\n",
    "texture_loss = TextureLoss(loss_type=\"OT\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data\n",
    "batch = next(iter(val_dataloader))\n",
    "cond_images = batch['image']\n",
    "weights = batch['weights']\n",
    "text_names = batch['texture']\n",
    "base_enc_cond_images = base_encoder(cond_images.to('cuda'))\n",
    "comp_enc_cond_images = comp_encoder(cond_images.to('cuda'))\n",
    "\n",
    "# Sample the generated weights\n",
    "base_model_nca_weights = baseline.sample(num_steps=50, cond=base_enc_cond_images, seed=42)\n",
    "comp_model_nca_weights = comparison_model.sample(num_steps=50, cond=comp_enc_cond_images, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images from the ground truth weights\n",
    "weights2image_gen.generate(weights[:100])\n",
    "gt_gen_images = weights2image_gen.generated_images\n",
    "weights2image_gen.generated_images = []\n",
    "\n",
    "# Generate images from the baseline model's predicted weights\n",
    "weights2image_gen.generate(base_model_nca_weights[:100])\n",
    "base_pred_gen_images = weights2image_gen.generated_images\n",
    "weights2image_gen.generated_images = []\n",
    "\n",
    "# Generate images from the comparison model's predicted weights\n",
    "weights2image_gen.generate(comp_model_nca_weights[:100])\n",
    "comp_pred_gen_images = weights2image_gen.generated_images\n",
    "weights2image_gen.generated_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT OT median: 4.92, Base OT median: 8.12, Comp OT median: 8.05\n"
     ]
    }
   ],
   "source": [
    "# Compute the OT distance between the ground truth and the predicted images\n",
    "gt_ot_losses = texture_loss(cond_images.to('cuda')[:100], torch.stack(gt_gen_images))\n",
    "base_ot_losses = texture_loss(cond_images.to('cuda')[:100], torch.stack(base_pred_gen_images))\n",
    "comp_ot_losses = texture_loss(cond_images.to('cuda')[:100], torch.stack(comp_pred_gen_images))\n",
    "\n",
    "# Compute the median OT distance for each model\n",
    "gt_ot_med = np.median([ot_loss.item() for ot_loss in gt_ot_losses])\n",
    "base_ot_med = np.median([ot_loss.item() for ot_loss in base_ot_losses])\n",
    "comp_ot_med = np.median([ot_loss.item() for ot_loss in comp_ot_losses])\n",
    "\n",
    "print(f'GT OT median: {gt_ot_med:.2f}, Base OT median: {base_ot_med:.2f}, Comp OT median: {comp_ot_med:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a52f375e40b4aee9adcaf0e07d2c4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=99), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(idx: int):\n",
    "    # Transform tensors to PIL images\n",
    "    cond_image = to_pil_image(cond_images[idx])\n",
    "    gt_gen_image = to_pil_image(gt_gen_images[idx])\n",
    "    base_pred_gen_image = to_pil_image(base_pred_gen_images[idx])\n",
    "    comp_pred_gen_image = to_pil_image(comp_pred_gen_images[idx])\n",
    "\n",
    "    # Get the OT distance for each image\n",
    "    gt_ot_loss = gt_ot_losses[idx].item()\n",
    "    base_ot_loss = base_ot_losses[idx].item()\n",
    "    comp_ot_loss = comp_ot_losses[idx].item()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axs[0].imshow(cond_image)\n",
    "    axs[0].set_title('Condition')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(gt_gen_image)\n",
    "    axs[1].set_title(f'NCA, OT distance: {gt_ot_loss:.2f}')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(base_pred_gen_image)\n",
    "    axs[2].set_title(f'Baseline, OT distance: {base_ot_loss:.2f}')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    axs[3].imshow(comp_pred_gen_image)\n",
    "    axs[3].set_title(f'Comparison, OT distance: {comp_ot_loss:.2f}')\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    # Set figure title\n",
    "    fig.suptitle(f'Baseline = Gram Encoder, Comparison = Vision Transformer, XA')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(show_image, idx=widgets.IntSlider(min=0, max=len(gt_ot_losses)-1, step=1, value=0));"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
